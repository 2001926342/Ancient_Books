import os
from dataclasses import asdict
import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.utils import logging
from langchain.llms.base import LLM
from typing import Any, List, Optional
from langchain.callbacks.manager import CallbackManagerForLLMRun
from interface import GenerationConfig, generate_interactive
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA, LLMChain
from langchain.vectorstores import Chroma
from langchain.embeddings.huggingface import HuggingFaceEmbeddings

logger = logging.get_logger(__name__)
print('pip install modelscope websockets')
os.system(f'pip install modelscope websockets==11.0.3')

base_path = './model/Ancient_Books'
os.system(f'git clone hhttps://openxlab.org.cn/models/detail/element/Ancient_Books.git {base_path}')
os.system(f'cd {base_path} && git lfs pull')

gradient_text_html = """
<style>
.container {
    position: relative;
    /* å¯èƒ½éœ€è¦è°ƒæ•´çš„é«˜åº¦ï¼Œä»¥é¿å…å†…å®¹é‡å  */
    padding-top: 50px; 
}

.gradient-text {
    font-weight: bold;
    background: -webkit-linear-gradient(left, red, orange);
    background: linear-gradient(to right, red, orange);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    display: inline;
    font-size: 3em;
    /* ä½¿ç”¨ç›¸å¯¹å®šä½å¹¶ä¸Šç§» */
    position: relative;
    top: -115px;
}
</style>
<div class="container">
    <div class="gradient-text">ç”˜è‚ƒæ”¿æ³•å¤§å­¦å¤ç±è§£è¯»</div>
</div>
"""
st.markdown(gradient_text_html, unsafe_allow_html=True)

class InternLM_LLM(LLM):
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None

    def __init__(self, model, tokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        self.model = model
        self.model = self.model.eval()

    def _call(self, prompt: str, stop: Optional[List[str]] = None,
              run_manager: Optional[CallbackManagerForLLMRun] = None,
              **kwargs: Any):
        system_prompt =  """
            ä½ æ˜¯å¤ç±è§£è¯»åŠ©æ‰‹ï¼Œç²¾é€šå¤ä»£æ–‡çŒ®å’Œå…¸ç±ï¼Œå¯ä»¥æä¾›å…³äºå¤ç±è§£è¯»ã€å¤æ–‡ç¿»è¯‘å’Œå†å²æ–‡åŒ–èƒŒæ™¯çš„ä¸“ä¸šå»ºè®®å’Œä¿¡æ¯ã€‚æ— è®ºæ˜¯æƒ³äº†è§£å¤ç±ä¸­çš„æ™ºæ…§ï¼Œè¿˜æ˜¯å¯»æ‰¾ç‰¹å®šå¤æ–‡çš„ç¿»è¯‘å’Œæ³¨é‡Šï¼Œéƒ½èƒ½æä¾›å¸®åŠ©ã€‚
            """
        messages = [(system_prompt, '')]
        response, history = self.model.chat(self.tokenizer, prompt, history=messages)
        return response

    @property
    def _llm_type(self) -> str:
        return "InternLM"

def load_chain(model, tokenizer):
    embeddings = HuggingFaceEmbeddings(model_name="/group_share/Ancient_Books/model/sentence-transformer")
    persist_directory = '/group_share/Ancient_Books/dataset/vector_db/chroma'
    vectordb = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    retriever_chroma = vectordb.as_retriever(search_kwargs={"k": 3})
    llm = InternLM_LLM(model, tokenizer)
    template = """ä½ ã€ç”˜è‚ƒæ”¿æ³•å¤§å­¦å¤ç±è§£è¯»åŠ©æ‰‹ã€‘å¯ä»¥å‚è€ƒä»¥ä¸‹ä¸Šä¸‹æ–‡è¿›è¡Œæ€è€ƒï¼Œå¹¶å›ç­”æœ€åçš„é—®é¢˜ã€‚ä¸è¦è¡¨æ˜æ€è€ƒè¿‡ç¨‹ï¼Œç›´æ¥è¿”å›ç­”æ¡ˆã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
    æ¡ˆã€‚è¯·æä¾›è¯¦ç»†å¹¶ä¸”ç»“æ„æ¸…æ™°çš„å›ç­”ï¼Œå¹¶å°½é‡é¿å…ç®€å•å¸¦è¿‡é—®é¢˜ã€‚
    {context}
    é—®é¢˜: {question}
    æœ‰ç”¨çš„å›ç­”:"""
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"], template=template)
    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever_chroma, return_source_documents=True,
                                           chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})
    return qa_chain

def on_btn_click():
    if "messages" in st.session_state:
        del st.session_state.messages

@st.cache_resource
def load_model():
    model = (AutoModelForCausalLM.from_pretrained(base_path, trust_remote_code=True).cuda())
    tokenizer = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True)
    return model, tokenizer


def prepare_generation_config():
    with st.sidebar:
        st.title("ç”˜è‚ƒæ”¿æ³•å¤§å­¦å¤ç±è§£è¯»åŠ©æ‰‹")
        st.subheader("ç›®å‰æ”¯æŒåŠŸèƒ½")
        st.markdown("- ğŸ’– å¤è¯—èµæ")
        st.markdown("- ğŸ’¬ æ–‡è¨€æ–‡")
        st.markdown("- ğŸ“Š æˆè¯­")
        st.markdown("- ğŸ“Š è®ºè¯­")
        st.markdown("- ğŸ“Š ç™¾å®¶å§“")
        with st.container(height=200, border=True):
            st.subheader("æ¨¡å‹é…ç½®")
            max_length = st.slider("Max Length", min_value=32, max_value=2048, value=2048)
            top_p = st.slider("Top P", 0.0, 1.0, 0.8, step=0.01)
            temperature = st.slider("Temperature", 0.0, 1.0, 0.7, step=0.01)
        st.button("Clear Chat History", on_click=on_btn_click)
    generation_config = GenerationConfig(max_length=max_length, top_p=top_p, temperature=temperature, repetition_penalty=1.002)
    return generation_config

user_prompt = 'user\n{user}\n'
robot_prompt = 'assistant\n{robot}\n'
cur_query_prompt = 'user\n{user}\nassistant\n'

def combine_history(prompt):
    messages = st.session_state.messages
    meta_instruction = ('ä½ æ˜¯ã€ç”˜è‚ƒæ”¿æ³•å¤§å­¦å¤ç±è§£è¯»åŠ©æ‰‹ã€‘ã€‚ä½ ä¼šåŒ…æ‹¬ä½†ä¸é™äºå”è¯—ã€å®‹è¯ã€è®ºè¯­ç­‰å¤ç±ï¼Œä½ è¿˜å¯ä»¥è®©æˆ‘æ–‡è¨€æ–‡ç¿»è¯‘ç­‰ã€‚'
                        'ã€ç”˜è‚ƒæ”¿æ³•å¤§å­¦å¤ç±è§£è¯»åŠ©æ‰‹ã€‘ can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.')
    total_prompt = f"<s>system\n{meta_instruction}\n"
    for message in messages:
        cur_content = message['content']
        if message['role'] == 'user':
            cur_prompt = user_prompt.format(user=cur_content)
        elif message['role'] == 'robot':
            cur_prompt = robot_prompt.format(robot=cur_content)
        else:
            raise RuntimeError
        total_prompt += cur_prompt
    total_prompt = total_prompt + cur_query_prompt.format(user=prompt)
    return total_prompt

def main():
    print('åŠ è½½æ¨¡å‹ä¸­...')
    model, tokenizer = load_model()
    qa_chain = load_chain(model, tokenizer)
    print('æ¨¡å‹åŠ è½½å®Œæ¯•.')
    user_avatar = 'assets/user.png'
    robot_avator = 'assets/logo.png'
    # st.title("ç”˜è‚ƒæ”¿æ³•å¤§å­¦å¤ç±è§£è¯»åŠ©æ‰‹")

    generation_config = prepare_generation_config()
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"], avatar=message.get("assistant")):
            st.markdown(message["content"])

    if prompt := st.chat_input("What is up?"):
        with st.chat_message("user", avatar='user',):
            st.markdown(prompt)
        real_prompt = combine_history(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt, "avatar": 'user'})

        with st.chat_message('robot', avatar=robot_avator):
            message_placeholder = st.empty()
            for cur_response in generate_interactive(
                    model=model,
                    tokenizer=tokenizer,
                    prompt=real_prompt,
                    additional_eos_token_id=92542,
                    **asdict(generation_config),
            ):
                message_placeholder.markdown(cur_response + 'â–Œ')
            message_placeholder.markdown(cur_response)
        st.session_state.messages.append({
            'role': 'robot',
            'content': cur_response,
            'avatar': robot_avator,
        })
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()
